<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>如何利用 BERT 提取句向量 - Zee Tsui</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" href="/favicon.ico">
  <link rel="canonical" href="/posts/2020/20200118-%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8-bert-%E6%8F%90%E5%8F%96%E5%8F%A5%E5%90%91%E9%87%8F/" />

  
  
  <link rel="stylesheet" href="/css/style.min.c813c458db6847576b9bd786addc081f1f218bc0b99cdfe79f2daf35df2e453a.css">
  

  
    
    <meta property="og:title" content="如何利用 BERT 提取句向量"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="/posts/2020/20200118-%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8-bert-%E6%8F%90%E5%8F%96%E5%8F%A5%E5%90%91%E9%87%8F/"/>
    
    
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:site" content="@zerostaticio"/>
    <meta name="twitter:creator" content="@zerostaticio"/>
  

  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;700&display=swap" rel="stylesheet"> 
</head>




<body class='page frame page-blog-single'>
  <div id="menu-main-mobile" class="menu-main-mobile">
    <ul class="menu">
        
        
            
                <li class="menu-item-home">
                    <a href="/">Home</a>
                </li>
            
        
            
                <li class="menu-item-blog">
                    <a href="/posts/">Blog</a>
                </li>
            
        
            
                <li class="menu-item-categories">
                    <a href="/categories/">Categories</a>
                </li>
            
        
            
            <li class="menu-item-gallery">
                <a href="/gallery">Gallery</a>
            </li>
            
        
            
                <li class="menu-item-log">
                    <a href="/log/">Log</a>
                </li>
            
        
            
                <li class="menu-item-about">
                    <a href="/pages/about/">About</a>
                </li>
            
        
    </ul>
</div>
  <div id="wrapper" class="wrapper">
    <div class='header'>
  
  <a></a>
  <div class="menu-main">
    <ul>
      
      
      
      <li class="menu-item-home">
        <a href="/">
          
          <span>Home</span>
        </a>
      </li>
      
      
      
      <li class="menu-item-blog">
        <a href="/posts/">
          
          <span>Blog</span>
        </a>
      </li>
      
      
      
      <li class="menu-item-categories">
        <a href="/categories/">
          
          <span>Categories</span>
        </a>
      </li>
      
      
      
      <li class="menu-item-gallery">
        <a href="/gallery">
          
          <span>Gallery</span>
        </a>
      </li>
      
      
      
      <li class="menu-item-log">
        <a href="/log/">
          
          <span>Log</span>
        </a>
      </li>
      
      
      
      <li class="menu-item-about">
        <a href="/pages/about/">
          
          <span>About</span>
        </a>
      </li>
      
      
    </ul>
  </div>
  <div id="toggle-menu-main-mobile" class="hamburger-trigger">
    <button class="hamburger">Menu</button>
  </div>
</div>
    
  <div class="blog">
    <div class="intro-article">
      <h1>如何利用 BERT 提取句向量<span class="dot">.</span></h1>
      
    </div>
    <div class="content">
      <p>BERT 在 NLP 方向中是一个十分具有里程碑的模型，那么如何通过 BERT 提取一个句子的句向量呢？</p>
<p>网络上的资料还是很多的，由于比赛要求只使用 Pytorch 框架，所以很多基于 TF 的教程和库就没办法用了。在查询了部分资料后，我终于总结出了一个提取的方法：</p>
<blockquote>
<p>这一篇总结我会尽力写得通俗易懂，一读就明白。</p>
</blockquote>
<!-- more -->
<h2 id="算法解释">算法解释</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">    <span class="kn">from</span> <span class="nn">pytorch_pretrained_bert</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertModel</span><span class="p">,</span> <span class="n">BertForMaskedLM</span>
</span></span></code></pre></div><p>导入 <code>torch</code> 和 <code>pytorch_pretrained_bert</code> 库，站在巨人的肩膀上，直接缩短大量的训练时间。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>  <span class="c1"># 导入预训练模型</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="c1"># tokenize 输入的文本</span>
</span></span><span class="line"><span class="cl">    <span class="n">indexed_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>  <span class="c1"># 向量化</span>
</span></span></code></pre></div><p>这一部分的代码主要是将输入的文本转化为 <strong>tokenized 的文本</strong>，然后再把该文本给<strong>向量化</strong>。</p>
<h3 id="问题来了什么是-tokenized-和向量化">问题来了，什么是 tokenized 和向量化？</h3>
<h4 id="tokenized">tokenized</h4>
<p>即<strong>标记化</strong>，把输入的句子进行标记划的操作，举个小例子：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;Hello world !&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="o">&gt;&gt;&gt;</span> <span class="p">[</span><span class="s1">&#39;hello&#39;</span><span class="p">,</span> <span class="s1">&#39;world&#39;</span><span class="p">,</span> <span class="s1">&#39;!&#39;</span><span class="p">]</span>
</span></span></code></pre></div><p>大家可能注意到了，为什么字母全部为小写了？这是因为我们使用的模型 <code>bert_base_uncased</code> 是不分大小写的，所以输出的 <code>tokenized_text</code> 会全由小写表示。</p>
<p>其实 BERT 还支持输入两个句子，但是需要放在同一个字符串中，并通过一个标记来表示一个句子的开始和结束，再举个小例子：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;[CLS] Hello world! [SEP] I</span><span class="se">\&#39;</span><span class="s1">m Xiao Ming. [SEP]&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="o">&gt;&gt;&gt;</span> <span class="p">[</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">,</span> <span class="s1">&#39;hello&#39;</span><span class="p">,</span> <span class="s1">&#39;world&#39;</span><span class="p">,</span> <span class="s1">&#39;!&#39;</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">,</span> <span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="s2">&#34;&#39;&#34;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;xiao&#39;</span><span class="p">,</span> <span class="s1">&#39;ming&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">]</span>
</span></span></code></pre></div><p>其中的 <code>[CLS]</code> 是句子的开始符，<code>[SEP]</code> 是句子的结束符，同时也可以作为下一个句子的开始符，所以第二句的 <code>[CLS]</code> 就可以省去。</p>
<h4 id="向量化">向量化</h4>
<p>向量化便是把生成的 <code>tokenized_text</code> 变为<strong>矩阵向量表示</strong>，从而便于后续的网络运算，举个例子：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;[CLS] Hello world world [SEP] I</span><span class="se">\&#39;</span><span class="s1">m Xiao Ming. [SEP]&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">indexed_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="o">&gt;&gt;&gt;</span> <span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">7592</span><span class="p">,</span> <span class="mi">2088</span><span class="p">,</span> <span class="mi">2088</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span> <span class="mi">1045</span><span class="p">,</span> <span class="mi">1005</span><span class="p">,</span> <span class="mi">1049</span><span class="p">,</span> <span class="mi">19523</span><span class="p">,</span> <span class="mi">11861</span><span class="p">,</span> <span class="mi">1012</span><span class="p">,</span> <span class="mi">102</span><span class="p">]</span>
</span></span></code></pre></div><p>通过这个例子可以发现，两个 world 生成的向量是<strong>相同</strong>的，即<strong>每一个单词或者标记都有对应的词向量</strong>，储存在一个预训练好的表中；</p>
<h2 id="让我们继续">让我们继续</h2>
<p>OK ，在解释完原理后，继续一通操作：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">segments_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">indexed_tokens</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">segments_ids</span><span class="p">])</span>
</span></span></code></pre></div><p><code>segments_ids</code> 是用来<strong>标记分句</strong>的，用 0 和 1 分开一个 <code>tokenized_text</code> 中的两个句子——一个句子为 0，另一个句子为 1。因为我一次只输入一个句子，所以就全设为 1，长度与 <code>tokenized_text</code> 相同（<strong>不相同会报错</strong>，这个需要注意）。</p>
<p>而下面的 <code>tokens_tensor</code> 和 <code>segments_tensors</code> 是把生成出来的两个向量矩阵给<strong>张量化</strong>。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p>初始化预训练模型 <code>bert-base-uncased</code>。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">tokens_tensor</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">segments_tensors</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span></span></code></pre></div><p>将矩阵和模型全部放入 <code>cuda</code> 中，即用 GPU 运算，加快矩阵运算的速度。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">encoded_layers</span><span class="p">,</span> <span class="n">pooled_output</span><span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">,</span> <span class="n">segments_tensors</span><span class="p">)</span>
</span></span></code></pre></div><p>将 <code>tokens_tensor</code> 和 <code>segments_tensors</code> 放入模型<strong>前向传播</strong>，计算出最后的输出。其中 <code>encoded_layers</code> 为每一层产生的输出，<code>pooled_output</code> 为<strong>最后输出的句向量</strong>，而句向量为我们最终所需要的矩阵向量。</p>
<p>至此，提取句向量的工作已经完成了，接下来就需要输入下层网络王城后续工作了，代码放在了下面。</p>
<h2 id="代码">代码</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">    <span class="kn">from</span> <span class="nn">pytorch_pretrained_bert</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertModel</span><span class="p">,</span> <span class="n">BertForMaskedLM</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">indexed_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="n">indexed_tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">segments_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">indexed_tokens</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">segments_ids</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">tokens_tensor</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">segments_tensors</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">encoded_layers</span><span class="p">,</span> <span class="n">pooled_output</span><span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">,</span> <span class="n">segments_tensors</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">pooled_output</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">encode</span><span class="p">(</span><span class="s1">&#39;[CLS] Hello world world [SEP] I</span><span class="se">\&#39;</span><span class="s1">m Xiao Ming. [SEP]&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p><img src="https://images.unsplash.com/photo-1579284118918-129d5735e9fd?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1329&amp;q=80" alt=""></p>

    </div>
    
    <link href="https://cdn.bootcss.com/KaTeX/0.10.2/katex.min.css" rel="stylesheet">
    <script src="https://cdn.bootcss.com/KaTeX/0.10.2/katex.min.js"></script>
    <script src="https://cdn.bootcss.com/KaTeX/0.10.2/contrib/auto-render.min.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
                  delimiters: [
                      {left: "$$", right: "$$", display: true},
                      {left: "$", right: "$", display: false}
                  ]
              });
        });
    </script>
  </div>

    <div class="footer">
  <div class="footer-social">
    <h1></h1>
    <p>Copyright © 2022 All Rights Reserved. Built by Zee with ❤.</p>
  </div>
</div>
  </div>

  

  

  
  <script type="text/javascript" src="/js/bundle.min.5993fcb11c07dea925a3fbd58c03c7f1857197c35fccce3aa963a12c0b3c9960.js"></script>
  

  
  

</body>
</html>