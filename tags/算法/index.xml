<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>算法 on Zee Tsui</title>
    <link>/tags/%E7%AE%97%E6%B3%95/</link>
    <description>Recent content in 算法 on Zee Tsui</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/%E7%AE%97%E6%B3%95/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MRO、装饰器的调用顺序——Python 中需要注意的细节</title>
      <link>/posts/2021/20211227-mro%E8%A3%85%E9%A5%B0%E5%99%A8%E7%9A%84%E8%B0%83%E7%94%A8%E9%A1%BA%E5%BA%8Fpython-%E4%B8%AD%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E7%BB%86%E8%8A%82/</link>
      <pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/2021/20211227-mro%E8%A3%85%E9%A5%B0%E5%99%A8%E7%9A%84%E8%B0%83%E7%94%A8%E9%A1%BA%E5%BA%8Fpython-%E4%B8%AD%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E7%BB%86%E8%8A%82/</guid>
      <description>1. 重载运算符的调用顺序 class Foo: def __init__(self, name): self._name = name def __str__(self): return self._name def __eq__(self, other): return other is self class Bar(Foo): def __eq__(self, other): return str(self) == str(other) class Foo1: def __init__(self, name): self._name = name def __str__(self): return self._name def __eq__(self, other): return str(self) == str(other) foo = Foo(&amp;#34;Hello&amp;#34;) bar = Bar(&amp;#34;Hello&amp;#34;) foo1 = Foo1(&amp;#34;Hello&amp;#34;) print(foo == bar) # True, 调用了 bar 的 __eq__ print(bar == foo) # True, 调用了 bar 的 __eq__ print(foo == foo1) # False, 调用了 foo 的 __eq__ print(foo1 == foo) # True, 调用了 foo1 的 __eq__ Python 中，如果 == 符号两端变量属于具有继承关系的类，则会优先调用子类的重载方法；如果 == 符号两端变量分别属于两个完全不同的类，则会调用左端类的重载进行判断（见代码中print部分的后两个）</description>
    </item>
    
    <item>
      <title>XML 转 JSON 的 TypeScript 简易实现</title>
      <link>/posts/2021/20211003-xml-%E8%BD%AC-json-%E7%9A%84-typescript-%E7%AE%80%E6%98%93%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Sun, 03 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/2021/20211003-xml-%E8%BD%AC-json-%E7%9A%84-typescript-%E7%AE%80%E6%98%93%E5%AE%9E%E7%8E%B0/</guid>
      <description>为什么重复写脚本？ 最近在参与的一个 TypeScript 项目上出现了一个功能，即通过读取 xml 文件的属性信息来检查特定的动作是否执行完成。当然作为一名合格的模块战士，最好的方式就是用第三方库来解析 xml 文件，但作为一个折腾玩家，自己实现的功能才更加具有可控性。
json 相较于 xml 更加通用和方便，虽然 npm 上已经有 sax 这个开源库提供现成的 xml 解析方案了，但这些方案由于需要兼容很多“阴间” xml 样例，不得已会存在很多匹配上的的冗余代码。因此，考虑到项目中的 xml 并不存在特殊格式，所以可以简化很多步骤，在执行速度以及体积大小上会存在一定的优势。
方案简要 这一次的算法实现，我并没有过多地考虑性能优化——这并不只是因为我偷懒——主要还是先实现一个可用的 Demo 为主。xml 的内容解析类似于文本分析，所以在实现时很自然地就往递归遍历想了，简单粗暴，效果拔群。具体思路就是正确找出标签的开头位置和结尾位置进行解析，然后中间的内容递归到下一层进行匹配解析。
从这个角度出发，要解决的问题就分为了三个：标签文本拆分，json 对象构建，内容解析。
标签文本拆分 xml 始终以 &amp;lt;&amp;gt; 为标签进行内容分割，夹着子文本和子标签。所以如果要正确进行后续的匹配，只需要按照 &amp;lt;&amp;gt; 标记进行内容上的分割即可，然后将生成的字符串列表交给下一层进行处理。最终的实现效果是这样的：
源文本： &amp;#34; &amp;lt;/param&amp;gt; Hello World!&amp;lt;a&amp;gt;test&amp;lt;/a&amp;gt;Cest la vie &amp;lt;/param&amp;gt;&amp;#34; // 使用 .trim() 去除两边多余的空格回车 目标字符串列表： [ &amp;#34;&amp;lt;/param&amp;gt;&amp;#34;, &amp;#34;Hello World!&amp;#34;, &amp;#34;&amp;lt;a&amp;gt;&amp;#34;, &amp;#34;test&amp;#34;, &amp;#34;&amp;lt;/a&amp;gt;&amp;#34;, &amp;#34;Cest la vie&amp;#34;, &amp;#34;&amp;lt;/param&amp;gt;&amp;#34; ] 这样便可以方便得按照标签为单位进行处理了，这一部分在实现上没有考虑太多，直接新开辟了一个容器（再次偷懒），然后双指针遍历一遍匹配 &amp;lt;&amp;gt; 存入内存即可。后续再根据该列表的内容以及对应的规则进行 json 对象构建和参数匹配。
json 对象构建 内容解析部分需要根据 xml 的文本内容来构建 json 结构。首先是捣鼓这个结构到底长啥样，不仅需要具有通用性，而且不能够丢失原文本的信息（能够反向还原回去）。</description>
    </item>
    
    <item>
      <title>【算法笔记】动态规划 &amp; 最长公共子序列</title>
      <link>/posts/2021/20210604-%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97/</link>
      <pubDate>Fri, 04 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/2021/20210604-%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97/</guid>
      <description>突然发现自己有很长一段时间没有学习和更新算法相关的内容了。当然这并不完全是因为懒，主要还是临近毕业事务较多没能顾得上，做题的也感觉生分了许多。
什么是最长公共子序列（LCS, Longest Common Sequence） 关于 LCS 网络上有两种不同的含义，一个是“最长公共字符串（Longest Common String）”，一个是“最长公共子序列（Longest Commo Sequence）”。二者存在一定的差别，本文单独就后者进行相关的讨论。
最长公共子序列是动态规划中的一个经典的问题，即给定序列 A 和序列 B，然后求出两个序列中最大的公共子序列的长度，且该公共子序列中的元素可以不相邻。这类型的题目如果使用暴力解法一般都会直接顶满指数级的 $O(n^2)$ 复杂度，因此一般都会使用动态规划的写法来求解。
问题解法 LeetCode 上的对应题目见【1143. 最长公共子序列】。
首先介绍一下常规的暴力解法，直接通过递归来遍历所有可能的情况。代码如下：
def lcs(s1, s2): if s1 == &amp;#34;&amp;#34; or s2 == &amp;#34;&amp;#34;: return 0 elif s1[-1] == s2[-1]: return lcs(s1[:-1], s2[:-1])+1 else: return max(lcs(s1, s2[:-1]), lcs(s1[:-1], s2)) 这一写法显然存在着不足，即递归的部分会大量的重复，从而导致时间上的开销过大。因此，我们引入动态规划算法并建立矩阵进行相关数据的存储。假设两个字符串分别为 text1 和 text2，并定义dp[i][j] 为 text1 在 [0, i] 范围内的子串和 text2 在 [0, j] 范围内的子串的最长公共子序列。
状态转移：如果 text1[i-1]==text2[j-1]，那么说明我们找到了公共子串中的一个字符，则 dp[i][j] = dp[i-1][j-1] + 1；否则，如果 text1[i-1]!</description>
    </item>
    
    <item>
      <title>【算法笔记】布隆过滤器</title>
      <link>/posts/2021/20210428-%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/</link>
      <pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/2021/20210428-%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/</guid>
      <description>话不多说，这次直接切入主题，介绍一种牺牲准确性的哈希加速方法——布隆过滤器。这种数据结构及其算法原理较为简单，所以内容不会过于复杂。
什么是布隆过滤器（Bloom Filter） 布隆过滤器（英语：Bloom Filter）是1970年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。
上面是来自维基百科的介绍，详情可见布隆过滤器 - 维基百科。
说白了，布隆过滤器就类似与 set() 的操作，可以用于匹配特定键是否在集合之中。其原理也很简单，就是使用了多个哈希函数对键进行哈希得出多个值，然后将内存中对应位置设置为 1，如下图所示（来源见水印）：
当我们有一个 obj1 和 obj2 时，首先用 3 个 Hash 函数进行哈希得出对应的地址，然后依次设为 1，即可实现写入布隆过滤器的过程。
归纳一下具体的算法步骤：
初始化 k 个 Hash 函数，同时保证其能够分别哈希到不同的内存位置； 初始化一个长度为 n 比特的数组（bitarray/bitmap），每个位都设置为 0； 加入 key 时，先散列出 k 个地址，并将该位置的数据设置为 1； 查找 key 时，同样散列出 k 个地址，然后查询内存对应的内存位置，若全为 1 可认为在集合中，若有 0 则一定不在集合中。 该算法的缺点十分明显，因为位置存在重复性，所以不能够进行删除操作；同时，由于查询出来的内存位置此时可能已经有了其他的键 Hash 出来的 1，所以全为 1 的时候存在一定的误判率——当然，有 0 就已经可以证明该键并没有经过哈希了，不会发生误判。
这时候相信很多人会有一个疑问：为什么不用直接用哈希表呢？正常情况下，为了保证哈希表 $O(1)$ 的复杂度以及防止哈希冲突，哈希表的存储效率通常在 50% 以下，这就意味着哈希表往往都会较大，而布隆过滤器不用存键值，而且更加“紧凑”，所以可以在相对于小的内存里提供较好的查找性能。但代价就是存在一定的误判率，业务中也需要进行该方面的考量。
当数据量较小的时候，直接使用哈希表便可以优雅地解决问题了；此时使用布隆过滤器反而还需要承担额外不必要的错误率，得不偿失。
常用场景 实际的应用场景中，布隆过滤器广泛地应用于网页黑名单系统、垃圾邮件过滤系统、网页 URL 去重、垃圾邮件识别、大集合中重复元素的判断和缓存穿透等。下面是一些详细的场景：
数据库防止穿库。 Google Bigtable，HBase 和 Cassandra 以及 Postgresql 使用BloomFilter来减少不存在的行或列的磁盘查找。避免代价高昂的磁盘查找会大大提高数据库查询操作的性能。 业务场景中判断用户是否阅读过某视频或文章，比如抖音或头条，当然会导致一定的误判，但不会让用户看到重复的内容。 缓存宕机、缓存击穿场景，一般判断用户是否在缓存中，如果在则直接返回结果，不在则查询db，如果来一波冷数据，会导致缓存大量击穿，造成雪崩效应，这时候可以用布隆过滤器当缓存的索引，只有在布隆过滤器中，才去查询缓存，如果没查询到，则穿透到db。如果不在布隆器中，则直接返回。 WEB拦截器，如果相同请求则拦截，防止重复被攻击。用户第一次请求，将请求参数放入布隆过滤器中，当第二次请求时，先判断请求参数是否被布隆过滤器命中。可以提高缓存命中率。Squid 网页代理缓存服务器在 cache digests 中就使用了布隆过滤器。Google Chrome浏览器使用了布隆过滤器加速安全浏览服务 Venti 文档存储系统也采用布隆过滤器来检测先前存储的数据。 SPIN 模型检测器也使用布隆过滤器在大规模验证问题时跟踪可达状态空间。 （摘自愚公要移山的答案）</description>
    </item>
    
    <item>
      <title>用 Python 实现一个可自动识别的文件夹单向同步功能</title>
      <link>/posts/2021/20210327-%E7%94%A8-python-%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%AF%E8%87%AA%E5%8A%A8%E8%AF%86%E5%88%AB%E7%9A%84%E6%96%87%E4%BB%B6%E5%A4%B9%E5%8D%95%E5%90%91%E5%90%8C%E6%AD%A5%E5%8A%9F%E8%83%BD/</link>
      <pubDate>Sat, 27 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/2021/20210327-%E7%94%A8-python-%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8F%AF%E8%87%AA%E5%8A%A8%E8%AF%86%E5%88%AB%E7%9A%84%E6%96%87%E4%BB%B6%E5%A4%B9%E5%8D%95%E5%90%91%E5%90%8C%E6%AD%A5%E5%8A%9F%E8%83%BD/</guid>
      <description>写在前面：这是我对类 unison 这样猝发式同步工具的一种复现尝试；因为我平时主要就是在一个文件夹内进行操作，所以算法更为简单，并没有实现针对单个文件进行处理的双向同步。 我如今已经使用了更为高效易用的 FreeFileSync 去进行文件同步和备份操作，该程序已经归档。
为什么我想写这个算法 在日常生活以及学习中，文件夹同步这一操作自然是必不可少的，一个方便使用的文件同步软件可以很好的对这种进行操作。通过我以前的文章可以了解到，我之前一直都在使用 Unison 来进行本地以及 WSL 不同的文件夹间的同步。而后来由于自己的问题，更换电脑后，由于 Unison 麻烦的配置以及自己对于文件夹同步的想法，我还是想尝试自己写一个文件夹同步的算法。
文件夹同步的方式有两种，一种是 rsync 那样的单向同步，一种是 Unison 一样的双向同步（Unison 也是基于 rsync 的）；当然我更需要的是双向的，所以这就不由得涉及到了很多的同步问题，最后我还是改成实现一个自动识别主文件夹的单向同步，这同样也可以达到双向同步的效果。
算法细节 算法很简单，总共分为两个层面。首先是同步的工具，我直接使用了 rsync 来进行传输的操作，然后同步两个或多个文件夹即可；第二个部分就是判断哪个文件夹为同步的主文件夹，这一部分使用了文件的时间戳作为指纹保存，在同步之后，如果有与保存的指纹不同，则可判定其做了修改，即新文件夹。
时间戳生成算法 def timeset(path, data: dict, prefix: str): source = os.getcwd() path_list = os.listdir(path) os.chdir(path) for n in path_list: if os.path.isfile(n): if n != &amp;#39;syncing.json&amp;#39;: data[os.path.join(prefix, n)] = int(os.path.getmtime(n)) else: timeset(n, data, os.path.join(prefix, n)) os.chdir(source) return data 而如果在数据中没有指纹，则两个文件夹为新文件夹，此时直接取时间戳最大的文件夹。在判断完哪一个文件夹为同步的主文件夹之后，直接进行 rsync 同步即可。
算法源码 当然，为了保证该脚本的可用性，文件地址判定以及信息确认之类的都采用了严谨的写法，从而保证不会出现错误。
def sync_folder_local(*args, main=None): # WARNING: This function only works correctly with single-folder editing # It would have a bug occured when multi folders were edited database = load_json(f&amp;#39;{ os.</description>
    </item>
    
    <item>
      <title>【算法笔记】基于用户投票的排序算法</title>
      <link>/posts/2021/20210224-%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E6%8A%95%E7%A5%A8%E7%9A%84%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/2021/20210224-%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E6%8A%95%E7%A5%A8%E7%9A%84%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</guid>
      <description>这是一篇拖了很久的文章，其实很早之前便想把它写出来，但无奈只能拖到了现在。希望现在趁着还有些闲心，可以把以前记录过的坑全给填齐了。
基于用户投票的排序算法十分多样，不同社交平台的机制（点赞，点踩，热度等）会基于不同变量的排序算法。为了保证算法的普遍适用性，文章仅对一般的基于用户投票的算法模型进行介绍，全文使用“主题”作为排序的元素。
牛顿冷却算法 这是一个常用的基于时间和投票的排序算法，使用了指数式衰减的牛顿冷却公式为基础。其最为直观的结果就是投票高且时间较近的主题的排名会更为靠前，然后随着时间的增加，该主题便会“冷却”，排名逐渐降低。
公式如下：
$$T&amp;rsquo;(t)=-\alpha(T(t)-H)$$
$T(t)$：温度 $T$ 的时间 $t$ 的函数 $T&amp;rsquo;(t)$：温度 $T$ 的时间 $t$ 的导数 $H$ 代表室温，$T(t)-H$ 就是当前温度与室温之间的温差 常数 $\alpha(\alpha&amp;gt;0)$：室温与降温速率之间的比例关系，不同的物质有不同的 $\alpha$ 值。 基于这样的式子，我们可以进一步将其化简，得出我们最终在排序算法中所使用的式子：
$$T=T_0e^{-\alpha(t-t_0)}$$
$T$ 和 $t$：当前时刻的温度和时间 $T_0$ 和 $t_0$：之前的某一时刻的温度和时间 $\alpha$ ：冷却系数，一般根据需求计算得出，且 $\alpha$ 越大，同一时间衰减的速度越快 可以看出，牛顿冷却算法在需要考虑时间和票数两种因素的情况下，可以做一个很好的权衡。
威尔逊区间 因为“牛顿冷却算法”的指数衰减特性，其只适用于一段时间内的排序，而“威尔逊区间” 则适用于对整体的主题进行排序。
为什么需要“威尔逊区间”算法来进行排序呢，直接赞的数量减去踩的数量不好吗？这就涉及到样本空间的问题了。
假设现在有两个主题，一个有 500 赞和 450 踩，另一个有 60 赞和 0 踩，那么按照直观的解法 $60 - 0 &amp;gt; 500 - 450$，第二个放在了前面。实际上这样并不合理，因为明显第一个主题的讨论更高，这便意味着该主题的权重应该更高，更具有代表性，所以理应让第一个主题排序靠前。同理，$Score=\frac{赞成票}{总票数}$ 在有很多小样本的情况下也十分不准确。
基于这样的想法，有了下面这个太长不看的公式：
$$\frac{\bar{p}+\frac{1}{2n}z_{1-\frac{\alpha}{2}}^2\pm z_{1-\frac{\alpha}{2}}\sqrt{\frac{\bar{p}(1-\bar{p})}{n}+\frac{z_{1-\frac{\alpha}{2}}^2}{4n^2}}}{1+\frac{1}{n}z_{1-\frac{\alpha}{2}}^2}$$
$\bar{p}$：赞成票比例 $n$：样本大小 $z_{1-\frac{\alpha}{2}}$：对应某个置信水平的 $z$ 统计量（参考概率论） 通过式子我们可以看出，最大值为 $\frac{\bar{p}+\frac{1}{2n}z_{1-\frac{\alpha}{2}}^2+z_{1-\frac{\alpha}{2}}\sqrt{\frac{\bar{p}(1-\bar{p})}{n}+\frac{z_{1-\frac{\alpha}{2}}^2}{4n^2}}}{1+\frac{1}{n}z_{1-\frac{\alpha}{2}}^2}$，最小值为 $\frac{\bar{p}+\frac{1}{2n}z_{1-\frac{\alpha}{2}}^2-z_{1-\frac{\alpha}{2}}\sqrt{\frac{\bar{p}(1-\bar{p})}{n}+\frac{z_{1-\frac{\alpha}{2}}^2}{4n^2}}}{1+\frac{1}{n}z_{1-\frac{\alpha}{2}}^2}$（就是取了个正负号）。当 n 足够大时，最小值便会趋向于 $\bar{p}$，反之在样本量小时，该值便会远远小于 $\bar{p}$。这就达到了样本量大的权重更高这一要求。</description>
    </item>
    
    <item>
      <title>【算法笔记】meet-in-the-middle 算法</title>
      <link>/posts/2021/20210223-%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0meet-in-the-middle-%E7%AE%97%E6%B3%95/</link>
      <pubDate>Tue, 23 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/2021/20210223-%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0meet-in-the-middle-%E7%AE%97%E6%B3%95/</guid>
      <description>meet-in-the-middle 算法（又称折半搜索、双向搜索），属于一种优化的 DFS 或 BFS 算法。同分治算法近似，它将问题进行了拆分，然后进行合并归一，得出最后的结果。这样做的好处是在穷举解的时候能够对很多情况进行剪枝，降低了时间复杂度。
n &amp;lt;= 40 的搜索类型题目一般都可以优化，本质上这是一种空间换时间的算法。
穷举类的问题当匹配条件越多的时候，其时间复杂度便会越大，所以可以通过将多个条件拆分匹配的方式来减小复杂度。 简单来说，这就是“分而治之”的一种手段。
常见类型 求和 这一部分可直接见「举个例子」部分。
双向搜索 这在关系网处理和图的路径规划中经常使用，且在寻路问题中表现很好。算法会同时从两个节点开始搜索，并且看什么时候这两个搜索的边界相遇。这个可以将需要扩展的节点降低到 $O(p^{k/2})$。
2DES 破解 DES 算法为密码体制中的对称密码体制，又被称为美国数据加密标准，是 1972 年美国 IBM 公司研制的对称密码体制加密算法。明文按 64 位进行分组，密钥长 64 位，密钥事实上是 56 位参与 DES 运算（第 8、16、24、32、40、48、56、64 位是校验位， 使得每个密钥都有奇数个1），分组后的明文组和 56 位的密钥按位替代或交换的方法形成密文组的加密方法。
DES 算法如今之所以被淘汰，是因为秘钥空间太小。其密钥的 $2^{56}$ 种可能性在以前是很难穷举破解的，但如今随着算力的发展，把所有可能的秘钥遍历一遍也是可以的（参考 BTC 挖矿）。在这之后的 DES 算法便开始显得力不从心，亟待升级，于是就有了 3DES 算法——使用 3 个密钥进行 3 次 DES 加密运算。
2DES 去哪儿了？答案是 DES 算法过后直接提升到了 3DES，直接把 2DES 给跳过了。理论上来说 2DES 具有 $2^{102}$ 的秘钥空间，已足够使用，但是为什么不用呢，原因就在于 meet-in-the-middle 算法为基础的中间人攻击（Diffile-Hellman 发明）——信息论课堂上的老朋友了，在这儿只用简单的语言介绍其原理。
假设我需要通过一组 2DES 的明文与密文破解出秘钥，我并不需要遍历 $2^{102}$ 整个秘钥空间，而是使用明文枚举 $2^{56}$ 个秘钥加密，得到 $2^{56}$ 个中间值并存入哈希表；然后使用密文枚举 $2^{56}$ 个秘钥并与明文的哈希库做对比，得到的所有值中一定有一个与之前加密所得到的相等，即 $E_{ki}(p) = D_{kj}(s)$，meet-in-the-middle 结束。这整个破解过程中不难发现，只要有足够大的空间用于存储哈希表，2DES 破解密码的时间仅仅相当于破解 2 次 DES。</description>
    </item>
    
    <item>
      <title>【算法笔记】动态规划 &amp; 二分搜索法</title>
      <link>/posts/2021/20210122-%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2%E6%B3%95/</link>
      <pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/2021/20210122-%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2%E6%B3%95/</guid>
      <description>动态规划（DP, Dynamic Programming）由于需要对问题进行抽象拆分，然后化简，因此具有一定的难度和乐趣。它属于一种算法设计的技巧，即对暴力遍历中的各种情况（重叠子问题）进行合理地剪枝，从而达到减小时间复杂度的目标。
动态规划的一般流程为：递归的暴力解法 → 带备忘录的递归解法 → 非递归的动态规划解法。也就是说，平时使用动态规划的方法来解决一个特定的问题时，并不是一蹴而就的，而是从暴力的问题出发进行一步步优化而来的。
此外，本文除了会对动态规划及其相关的应用进行阐述之外，还会介绍对动态规划问题进行优化的二分搜索法。
一个问题是该用递推、贪心、搜索还是动态规划，完全是由这个问题本身阶段间状态的转移方式决定的。
缓存、重叠子问题、记忆化 以 Fibonacci 数列为例，直接写出来的代码是这样的：
def fib(n:int) -&amp;gt; int: if n == 0: return 0 elif n == 1 or n == 2: return 1 else: return fib(n - 1) + fib(n - 2) 这是使用了递归的 Fibonacci 数列求解算法。因为是递归，所以该解法中一定会存在着重复部分，计算机还是会傻傻地对其重新计算一次，这便是需要进行优化剪枝的重叠子问题。我们可以在计算 fib(n) 之后，使用 LRU Cache 或者创建一个列表来把 fib(n) 给缓存起来，这样就可以避免对其进行重复运算，从而节省时间。
这便是动态规划最基本的运用，即通过一张表作为“备忘录”，从而减少递归的次数。
原本暴力求解的时间复杂度为 $O(n^2)$，经过优化后的时间复杂度仅为 $O(n)$。但是空间复杂度增大为了 $O(n)$，这本质上就是一个空间换时间的过程。
举个例子 下面用几个例子来归纳动态规划的原理：
凑零钱问题：「322.零钱兑换」 这一题其实可以通过递归的方法来解决，但是自从 LeetCode 改了测试集之后，递归解法就一定会超时（无论如何神优化都会超），所以只得从 DP 来入手。此题和背包问题一致，即求在固定的容量下能够装得下的最小（最大）数量。方法在于通过一个 DP Table 的形式来对最小硬币数量来进行记录，首先需要分析它的转移方程，当容量为 1（amount == 1）时的最优值一致往下推，等于 2 等于 3 等于 4……，最后等于 amount 的时候便可以得出最终的结果了。</description>
    </item>
    
    <item>
      <title>【算法笔记】并查集</title>
      <link>/posts/2021/20210117-%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E5%B9%B6%E6%9F%A5%E9%9B%86/</link>
      <pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/2021/20210117-%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%E5%B9%B6%E6%9F%A5%E9%9B%86/</guid>
      <description>最近在 LeetCode 的每日一题中，大量出现了使用并查集进行算法设计的题目，如「684.冗余连接」、「803.打砖块」、「947.移除最多的同行或同列石头」。并查集这一点本身并不难学习，但有时候在题目中不容易发现对应的连接关系。
什么是并查集？ 并查集是一种树形的数据结构，旨在通过特定的规则，将属于同一类别的点归于一个树中，构成一个集合。其包括两种基本操作：
合并（Union）：将两个不同的集合合并为一个集合，或者将新的元素合入特定的集合 查找（Find）：确定两个元素是否在同一个集合中，或者确定某个元素处于哪一个子集 注意：虽然并查集是一个树形的数据结构，但考虑到内存以及时空复杂度，一般都会使用数组或者列表来实现，而非 Node 树结点。
建树初始化 首先，需要根据当前数据的规模来开辟空间，同时确保原始数据能够 One-Hot 地映射到创建的节点集合中，即创建的空间能够包括所有的原始数据。下面是样板代码（Python）：
class Union: def __init__(self, N): self.p = list(range(N)) # 初始化列表，每一个节点的值等于索引，其中 N 要根据原始数据确定 这一部分不难理解，即开辟一块额外的空间来保存并查集中的父子关系，因为现在还没有初始化关系，所以每一个点都指向自己的位置。接下来便是合并（Union），目的是输入数据形成并查集，样板代码如下：
def find(self, x): if x != self.p[x]: self.p[x] = self.find(self.p[x]) # 这里的递归是为了压缩路径，加快查找速度，下面会讲到 return self.p[x] find() 会返回该点所对应集合的根（父亲），如果两个点返回同样的根（父亲），则证明其处于同一个集合中。
一般情况下合并的样本代码如下：
def union(self, x, y): xr = self.find(x) # 找出 x 对应的根 yr = self.find(y) # 找出 y 对应的根 self.p[xr] = yr # 将 x 的根指向 y 的根，因为其属于同一个集合 不同的题目用到的数据不同，一般是某个点的坐标 (x, y) 之类。这类题往往都可以使用递归、DP 等方法解决，所以我经常会往递归和 DP 的方向去想，然后卡在某个状态上（暴风哭泣），从而忘了直接用更简单的并查集来解。</description>
    </item>
    
    <item>
      <title>图像相似度的比较算法</title>
      <link>/posts/2020/20200423-%E5%9B%BE%E5%83%8F%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%9A%84%E6%AF%94%E8%BE%83%E7%AE%97%E6%B3%95/</link>
      <pubDate>Thu, 23 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/2020/20200423-%E5%9B%BE%E5%83%8F%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%9A%84%E6%AF%94%E8%BE%83%E7%AE%97%E6%B3%95/</guid>
      <description>最近花了些时间研究了一下图像相似度算法，感觉还是挺有趣的，也解决了我以前的一些疑惑。
目前的识别算法主要有几种，第一个是感知哈希算法，其中包括均值哈希（aHash）、感知哈希（pHash）和差异值哈希（dHash）；第二种是 SIFT。而目前主要使用的是 SIFT 和 pHash。
下面简单介绍一下这几种算法的原理和优势。
均值哈希（aHash） 原理 缩放图片：将图片进行下采样至 8*8 的像素矩阵，共 64 个像素； 灰度图片：将图片转化为 256 阶的灰度图； 灰度算法有很多: 心理学灰度 $Gray=0.299\times R+0.587\times G+0.114\times B$ 平均值算法 $Gray=(R+G+B)/3$ 绿色值算法 $Gray=G$
二元化图片：将图片进行二元化转换； 这一步简单来说，就是先计算灰度均值，然后依次遍历原像素值，高于灰度值记为 1，低于记为 0，最后生成了一个 64 bits 的数列。
构造哈希：对 64 bits 的数列进行哈希，生成“感知指纹”； 对比指纹：对两幅图片的“指纹”计算汉明距离，距离越小则相似度越高。 算法特点 aHash 的算法简单粗暴，速度快。但是由于对图片进行了下采样和二值化，所以自然丢失了非常多的数据，精确度也就很低。
感知哈希（pHash） 原理 缩放图片：将图片进行下采样，生成正方形的图片（图片一般大于 8*8，3*32）； 灰度图片：该步骤与上同； 计算 DCT（离散余弦变换）：DCT 计算将图片按照频率分布进行分解； 裁剪 DCT：经过 DCT 操作后，会生成一个 32*32 的频域矩阵，我们只需要保存左上角的 8*8 即可，这一部分是图片的低频； 二元化矩阵：对上一步的矩阵进行处理，算法同样是计算平均值然后二元化； 构造哈希：对 64 bits 的数列进行哈希，生成“感知指纹”； 对比指纹：对两幅图片的“指纹”计算汉明距离，距离越小则相似度越高。 DCT 的算法相较复杂，主要用于数据或图像的压缩，能够将空域的信号转换到频域上，具有良好的去相关性的性能。 一维 DCT 变换： $$ F(u)=c(u)\sum_{i=0}^{N-1}f(i)cos[\frac{(i+0.</description>
    </item>
    
    <item>
      <title>AC 自动机的原理及实现</title>
      <link>/posts/2020/20200412-ac-%E8%87%AA%E5%8A%A8%E6%9C%BA%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Sun, 12 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/2020/20200412-ac-%E8%87%AA%E5%8A%A8%E6%9C%BA%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/</guid>
      <description>在最近水论坛的时候，我发现了一个网友友问的这样一个问题：
数千万篇文章，寻找其中包含成语的句子。成语有数万条。 目前没有对文章内容建立过全文索引，鉴于这个事情是一次性的，为此搞个索引可能也成本过高。暂时的解决方案是，把成语都放在一条 re.compile(&amp;lsquo;乌合之众|鸡犬相闻|&amp;hellip;&amp;rsquo;)里面去搜索文章，但效率总觉得不理想。 求教，是否可能有更高效的解决方案。
我写过爬虫，也做过相关的一些算法，对于数据匹配无非就是正则表达式了，毕竟爬的数据量都还不大，效率方面的感知不强。
但是正则的缺点就是效率低，所以这个有大规模数据匹配需求的网友就遇到了低效的问题。这时，底下的一条评论靠着简单的一个词解决了这个问题：
“AC 自动机。”
AC 自动机是什么？ AC 自动机的原理简单来说，就是根据输入的多个模式串去建立一个树模型，然后再根据这个模型作字符串匹配。那它具体是什么呢？
在聊 AC 自动机之前，我们还需要了解两个基本算法：KMP 和 trie 树。
KMP KMP 算法是一种改进的字符串匹配算法，能够高效地进行串匹配。
传统的一种字符串匹配算法是类似于滑窗一样地进行逐个比对，如下：
T: b a a b a b c x W: a b a Wrong. T: b a a b a b c √ x W: a b a Wrong. T: b a a b a b c √ √ √ W: a b a Right. 后来有人发现了，这样匹配多没效率啊，不如匹配错误的时候，跳过一些字符，可以减少匹配计算量。</description>
    </item>
    
    <item>
      <title>数据压缩和信息熵</title>
      <link>/posts/2020/20200409-%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%92%8C%E4%BF%A1%E6%81%AF%E7%86%B5/</link>
      <pubDate>Thu, 09 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/2020/20200409-%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%92%8C%E4%BF%A1%E6%81%AF%E7%86%B5/</guid>
      <description>压缩的原理及有限性 在计算机导论中简要介绍过几种压缩的算法，原理十分简单，便是利用更短、熵更高的字符串来代替一些重复率高，熵较低的字符串，从而实现缩短字节量，减小文件的体积的目的。步骤大致如下：
得到文件的概率分布，统计频次高的以及频次低的部分； 结合统计结果对源文件重新编码，用短字符代替重复的长字符。 举个例子：
AAAAAAAAAA =&amp;gt; 10A // 用 3 个字符代表 10 个字符，压缩比 30% ABCABCABCABCABCABCABCABC =&amp;gt; 8ABC // 用 4 个字符代表 24 个字符，压缩比 16.7% 显而易见，越是重复率高的文件，就意味着能够压缩的量就越大，体积也自然能够越小；反之，如果一个文件的重复率低，也就越难压缩。所以每一个文件的熵不同，压缩比率也会不尽相同。
那么，压缩有一定的限度吗？ 当然，在信息论中我们学到过，无损压缩实现的前提是输出和输入必须要严格一一对应，也就是不能够出现多对一的映射。我们可以假设任意文件都能够压缩到 $n$ bits，那么就能够产生 $2^n$ 种可能的压缩结果。这便意味着如果有 $2^n+m$ 个不同的文件，就会导致有 $m+1$ 个文件出现了重复，所以压缩一定存在着极限。
压缩的极限 一个文件的压缩极限可以通过计算文件的平均信息熵大小，从而推导出来。当然这个最小值仅仅是理论最小值，并不一定能够达到，下面我将用浅显的方式做一些推导。
在不同的文件格式中可能存在着不同的计算方法，比如文本文件可能就会根据文本中的字符计算信息熵，图片会使用 RGB 排列来计算信息熵等。我这里使用了一个通用的方法——将任意文件按字节的形式进行读取，统计不同字节的信息熵。
计算平均信息熵的公式为：
$$H(X)=\sum P_n * \log(\frac{1}{P_n})$$
$P_n$ 为每一个字节在文件中出现的概率，计算方法如下：
$$P_n=\frac{N_{freq}}{N_{all}}$$
$N_{freq}$ 为该字节在文件中出现的次数，$N_{all}$ 文件的总字节量。
其中的 $log$ 在信息论中是默认以 2 为底的（在通信原理中是默认以 10 为底），通常会做省略处理。
最后求出来的 $H(X)$ 是文件每一个字节的平均信息熵，如果乘以总字节数就可以得到理论的压缩极限大小：
$$Size_{min}=H(X) * N_{all}$$
信息熵的含义 信息熵只反映内容的随机性，与内容本身无关。不管是什么文件，服从同样的概率分布就会得到同样的信息熵。 信息熵越大，表示占用的二进制位越长，可以表达更多的符号。即信息熵越大，信息量就越大，但这并不代表所获得的的信息越大。 代码实现 (Python) import sys from math import log from argparse import ArgumentParser from os.</description>
    </item>
    
  </channel>
</rss>
