<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>计算机 on Zee Tsui</title>
    <link>/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/</link>
    <description>Recent content in 计算机 on Zee Tsui</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Apr 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>谈谈我所认为的主流编程语言</title>
      <link>/posts/2020/20200422-%E8%B0%88%E8%B0%88%E6%88%91%E6%89%80%E8%AE%A4%E4%B8%BA%E7%9A%84%E4%B8%BB%E6%B5%81%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/</link>
      <pubDate>Wed, 22 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/2020/20200422-%E8%B0%88%E8%B0%88%E6%88%91%E6%89%80%E8%AE%A4%E4%B8%BA%E7%9A%84%E4%B8%BB%E6%B5%81%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/</guid>
      <description>呆在家里的时间越来越长了，这也给了我很多时间去做一些以前没有想过的思考。
所以趁着目前的学习阶段，我想谈谈我所认为的三种主流编程语言——C++、Java、Python。这三种语言有着不同的运行方式、不同的运用场景和不同的使用目的，也是我目前主要使用的三个编程语言。
C++ C++ 是我最早接触的语言之一了，早期的 C++ 更多地是 C 的一个不严谨的“超集”，你甚至可以在 C++ 的语法环境下编写 C 程序，两者相互兼容。后来的 C++ 17 / 18 反而越来越像“Python”，这也足以显示目前语言的一个发展方向。
的确如今的 C++ 复杂度已经过分高了，从而让开发者把更多的时间花在了指针以及内存管理等本可以自动处理的部分。这对于如今互联网企业的“敏捷开发”潮流来说十分不友好，如果贯彻 C++ 就需要花费大量时间搭建脚手架。
优点 优点显而易见，程序运行效率极高，接近于机器语言，并且相比于 C 有 OOP 的能力。所以这能够让 C++ 开发者在硬件层次上考虑问题，从而最大化利用硬件的性能。
所以这就特别适用于对运行速度要求很高，与系统底层相关的程序。
缺点 那它还有什么缺点呢？首当其冲的就是在开发过程中，工程师需要花费远超算法实现的时间（至少对于我来说是这样）去解决硬件资源管理和内存管理的问题，而这些问题往往与目前解决的问题无关。
第二点便是代码量和复杂的设计。这大概是历史原因，因为 C++ 一直在鼓励复杂的、精致的设计，导致了庞大的代码体积。
Java Java 是我最近学习的一种语言。期初我学习 Java 的兴趣并不高，但是后来的作业中用了一下，便继续深入了一点，现在大概还是初学者阶段吧。（笑）
因为我的 Java 经验并不是特别足，所以就简单对比一下 Java 与其他语言的优劣。
优点 优点很直观，就是“Write once, run anywhere”，这也是当初 Java 被设计出来的原因。因为 Java 需要先将代码编译成可供 JVM 运行的字节码，所以这也意味着程序可以完全不依赖与运行的平台，只要有 runtime 的运行环境即可。
其次便是 Java 的编程过程相较于 C++ 就轻松了很多，不仅比 C++ 小巧简单，而且有 GC，OOP，强类型，与 C 结合比较好。</description>
    </item>
    
    <item>
      <title>字符编码：ASCII、UTF8 和 Unicode</title>
      <link>/posts/2020/20200416-%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81asciiutf8-%E5%92%8C-unicode/</link>
      <pubDate>Thu, 16 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/2020/20200416-%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81asciiutf8-%E5%92%8C-unicode/</guid>
      <description>字符的编码格式有很多种，互联网上广泛存在着不同种类编码方式编码的文本文件。肯定有很多人在编程的时候遇到过读取文件出现乱码的问题，这很大程度上就是文件编码格式和读取时使用的编码格式不一致造成的，所以就经常需要根据文本编码类型来选择相应的编码格式读取文件。
字符编码是计算机技术的基石，所以了解编码格式的一些皮毛还是十分重要的。我再网络上查阅了相关的几篇文章，总结了主流编码格式 ASCII、UTF-8、Unicode 的相关知识点信息。
字节与编码 在了解不同的编码格式之前，我们还需要知道计算机是如何进行存储和编码的。
计算机内部处理和存储信息时，所有的信息都可以描述为一串特定的二进制信息流，其最小的单位为一个比特 $bit$ ，每一个比特都存在 $0$ 和 $1$ 两种状态；而 8 个比特便是一个字节 $byte$，可以表达出 $2^8=256$ 种状态。通过读取不同的状态，计算机就可以执行对应的操作，而这个把特定的一个状态对应到特定操作上的这个过程，就是编码。
下面贴一个比较官方的描述：
编码是信息从一种形式或格式转换为另一种形式的过程，也称为计算机编程语言的代码简称编码。用预先规定的方法将文字、数字或其它对象编成数码，或将信息、数据转换成规定的电脉冲信号。
所以，有了编码，我们就可以通过这样的方式写一套编码表，将不同的字符用对应的字节码来表示，从而让计算机“读懂”文字，并且显示在屏幕上，形成这一篇文章（笑）。
ASCII 编码 ASCII 码是最早的编码方式之一，早在上个世纪 60 年代，美国就制定了相关的一套字符编码表，并一直沿用至今。
ASCII 码中一共规定了 128 个字符，每一个字符通过 1 个字节来表示，但这 128 个符号只占用了一个字节的后 7 位，最前一位规定为 0。
对于英语国家来说，128 个字符就足够使用了，但是表示其他的语言是远远不够的——比如法语、希腊语等。（因为当时主要是在欧洲国家和北美洲国家使用，所以没有考虑到庞大的汉字系统）
因此后来 ASCII 码的标准被广泛使用，为了适应非英语语系国家以及数学家（误）的使用需求，码表还需要添加其他的非英语字符和符号，于是之前空出来的 1 比特就派上了用场，扩展 ASCII 问世。
非 ASCII 编码 不同的国家有不同的字母，128 个字符是囊括不完的。所以实际上高位的 128 个字符在不同的国家编码对应着不同的符号。比如 ASCII 码中，130 号字符在法语里是 é，而在希伯来语里是 ג，在其他的语言又会是另一个符号。
但是无论怎么改，都是高 128 位（128-255）不同，而低 128 位（0-127）所表示的符号全是一样的。
汉字由于是另一个语系，使用的符号十分之多，汉字就 10 万往上语法符号还和英文不一样，所以肯定需要更更多地字节去编码表示。例如简体中文常见的编码方式是 GB2312，每一个汉字需要两个字节来表示，理论上最多能够表示 $2^8\times2^8=65536$ 个符号。</description>
    </item>
    
  </channel>
</rss>
