<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>深度学习 on Zee Tsui</title>
    <link>/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 深度学习 on Zee Tsui</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Mar 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ASC20 比赛记录 - 3</title>
      <link>/posts/2020/20200303-asc20-%E6%AF%94%E8%B5%9B%E8%AE%B0%E5%BD%95-3/</link>
      <pubDate>Tue, 03 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/2020/20200303-asc20-%E6%AF%94%E8%B5%9B%E8%AE%B0%E5%BD%95-3/</guid>
      <description>ASC20 比赛因为疫情的原因，proposal 的提交延期了一个多月，所以中间我也休息了一段时间。最近随着诸多工作的完成，ASC 20 的比赛之旅也渐渐到了尾声。
题目 这一次的赛题是有关 NLP 方向的，要在只使用 Pytorch 框架的情况下完成 Cloze 下游任务的设计，并且以准确率作为最终的分数计算。
主办方并未提供 baseline，只提供了数据集，格式为 json，具体如下：
{ &amp;#39;article&amp;#39;: &amp;#39;...&amp;#39;, &amp;#39;options&amp;#39;: [[words * 4] * 20], &amp;#39;answers&amp;#39;: [&amp;#39;A&amp;#39;,&amp;#39;B&amp;#39;... * 20] } 目前的进展及思路 在之前的训练的过程中出现了一个比较恼人的问题，最早我采用的是 4 分类，但是网络会出现不收敛的问题，我尝试了许多的手段（正如之前所提到的那样）也是没有办法去解决。后来经过检查和与师兄的讨论才发现作 4 分类本身就比较难作拟合——可能根本就没有拟合，毕竟样本太少，每一部分的文字又太多，导致 embedding 出来的矩阵差距较小。
后来采用了 2 分类效果就好了很多，因此目前训练的成果已经成功追上了 1 队。而且由于 Transformers 在训练的时候能够自动判断 GPU 的环境使用分布式训练，所以还是很顺畅地做了分布式训练。</description>
    </item>
    
    <item>
      <title>ASC20 比赛记录 - 2</title>
      <link>/posts/2020/20200208-asc20-%E6%AF%94%E8%B5%9B%E8%AE%B0%E5%BD%95-2/</link>
      <pubDate>Sat, 08 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/2020/20200208-asc20-%E6%AF%94%E8%B5%9B%E8%AE%B0%E5%BD%95-2/</guid>
      <description>ASC20 比赛已经到了中段，是时候给最近的工作做一下总结了。
题目 这一次的赛题是有关 NLP 方向的，要在只使用 Pytorch 框架的情况下完成 Cloze 下游任务的设计，并且以准确率作为最终的分数计算。
主办方并未提供 baseline，只提供了数据集，格式为 json，具体如下：
{ &amp;#39;article&amp;#39;: &amp;#39;...&amp;#39;, &amp;#39;options&amp;#39;: [[words * 4] * 20], &amp;#39;answers&amp;#39;: [&amp;#39;A&amp;#39;,&amp;#39;B&amp;#39;... * 20] } 目前的进展及思路 之前所尝试过的预先填词的方法可行性不高，所以只能够将所有的词填进去。我们的算法是以每个空的四个选项去生成四句话，如果出现了一句话多个空的情况，则留空不填，然后以此生成了总共空数 * [ 4, 预填空句子 ] 的矩阵。
然后就得提取特征了，用 BERT 的预训练模型 bert-base-uncased 生成向量矩阵，其中每一句话都会生成一个 [1, 768] 的向量，然后存入 .npy 文件中。把答案的 ABCD 转换为 [1, 4] 的 onehot 向量，也存入另一个 .npy 文件中，供后面训练使用。
训练我们使用的是 BiLSTM + Attention Net，通过注意力机制来提高准确率。
存在问题 目前训练了几波，存在这一些问题：
out of memory 这个是老生常谈的问题了，主要的原因是 batch_size 设置得过大，从而导致显存使用过高。第二个是未终止之前的训练过程，之前通过 nohup 等手段运行的训练不会自动退出，需要主动 kill 掉。</description>
    </item>
    
    <item>
      <title>如何利用 BERT 提取句向量</title>
      <link>/posts/2020/20200118-%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8-bert-%E6%8F%90%E5%8F%96%E5%8F%A5%E5%90%91%E9%87%8F/</link>
      <pubDate>Sat, 18 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/2020/20200118-%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8-bert-%E6%8F%90%E5%8F%96%E5%8F%A5%E5%90%91%E9%87%8F/</guid>
      <description>BERT 在 NLP 方向中是一个十分具有里程碑的模型，那么如何通过 BERT 提取一个句子的句向量呢？
网络上的资料还是很多的，由于比赛要求只使用 Pytorch 框架，所以很多基于 TF 的教程和库就没办法用了。在查询了部分资料后，我终于总结出了一个提取的方法：
这一篇总结我会尽力写得通俗易懂，一读就明白。
算法解释 import torch from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM 导入 torch 和 pytorch_pretrained_bert 库，站在巨人的肩膀上，直接缩短大量的训练时间。
tokenizer = BertTokenizer.from_pretrained(&amp;#39;bert-base-uncased&amp;#39;) # 导入预训练模型 tokenized_text = tokenizer.tokenize(text) # tokenize 输入的文本 indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text) # 向量化 这一部分的代码主要是将输入的文本转化为 tokenized 的文本，然后再把该文本给向量化。
问题来了，什么是 tokenized 和向量化？ tokenized 即标记化，把输入的句子进行标记划的操作，举个小例子：
tokenized_text = tokenizer.tokenize(&amp;#39;Hello world !&amp;#39;) &amp;gt;&amp;gt;&amp;gt; [&amp;#39;hello&amp;#39;, &amp;#39;world&amp;#39;, &amp;#39;!&amp;#39;] 大家可能注意到了，为什么字母全部为小写了？这是因为我们使用的模型 bert_base_uncased 是不分大小写的，所以输出的 tokenized_text 会全由小写表示。
其实 BERT 还支持输入两个句子，但是需要放在同一个字符串中，并通过一个标记来表示一个句子的开始和结束，再举个小例子：
tokenized_text = tokenizer.tokenize(&amp;#39;[CLS] Hello world!</description>
    </item>
    
    <item>
      <title>ASC20 比赛记录</title>
      <link>/posts/2020/20200116-asc20-%E6%AF%94%E8%B5%9B%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/2020/20200116-asc20-%E6%AF%94%E8%B5%9B%E8%AE%B0%E5%BD%95/</guid>
      <description>ASC20 比赛在 1 月 6 号正式开始了，我在 1 月 10 号也正是接题开始了比赛的相关工作。因为这是我第一次以学校的名义参加全球性的赛事，所以特地记录下这段时间内的学习历程和思路。
题目 这一次的赛题是有关 NLP 方向的，要在只使用 Pytorch 框架的情况下完成 Cloze 下游任务的设计，并且以准确率作为最终的分数计算。
主办方并未提供 baseline，只提供了数据集，格式为 json，具体如下：
{ &amp;#39;article&amp;#39;: &amp;#39;...&amp;#39;, &amp;#39;options&amp;#39;: [[words * 4] * 20], &amp;#39;answers&amp;#39;: [&amp;#39;A&amp;#39;,&amp;#39;B&amp;#39;... * 20] } 学习 这一次的路线就非常直观了，需要走 NLP 方向，而 NLP 目前最为热门的网络模型就是 BERT。类似 ResNet 之于图像识别的网络，这是一个 NLP 中炙手可热，同时也十分常用的模型，而我们可以通过这个模型达到 word2vec 的效果，提取出文字中的向量信息，并输送到下层网络以完成下游任务。
所以我就打算使用 BERT 作为网络中的 embedding 部分，然后下层在进行 Cloze 的填空操作。在网上简单查找并且学习了 NLP的一些实现细节，我发现了 transformer 这个网红模型，并且有了许多的预训练模型，所以我便选择了以 transformer 为基础去完成题目。
目前的进展及思路 目前我已经基本看完了 transformer 中的 example 代码文件，并且对其工作原理有了基础的了解，可以开始准备数据集进行训练了。
我此外则有着以下的一些不成熟的小想法：
以我在观摩别人打比赛和自己比赛的经验，我会偏向于直接使用 pretrained 模型或者以其为底作迁移学习； 通过 BERT 提取特征并且进行猜词，然后与选项进行比对，如果不存在于选项中则需要通过比对 similarity，然后比较取最为相似的单词作为选择。 把选项放入原句中，然后通过 MRPC 下游任务计算句子的连贯性，取连贯性最高者。 考虑到上下文（比如 He or She 性别判断之类的选择），可能还需要通过另一个网络提取关键信息。 我针对第 2 点完成了基本的代码工作，但是目前就成果和效率来看，还是存在一定的不足，后续会对其进行改进，同时也会往多个方向进行探索~~，毕竟现在时间还多~~。</description>
    </item>
    
    <item>
      <title>防止过拟合和提高泛化能力的技巧</title>
      <link>/posts/2019/20191124-%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%8F%90%E9%AB%98%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E7%9A%84%E6%8A%80%E5%B7%A7/</link>
      <pubDate>Sun, 24 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/2019/20191124-%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%8F%90%E9%AB%98%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E7%9A%84%E6%8A%80%E5%B7%A7/</guid>
      <description>深度学习是一个很奇怪的过程，有人说它是“深度炼丹”，有人说它是一个黑盒，超参调整就是玄学。
事实上，我们不可能在深度学习中寻找到所谓的“最优解”，我们只能够在自己的能力范围之内寻找到局部的最优解，而如何提高自己网络模型的分类或者生成能力也自然有着许多的技巧。
数据 深度学习最需要的，就是数据，一个好的数据集可以让神经网络的训练事半功倍。在我看来，数据在保证质量的前提下多多益善，样本数据越多，模型拟合的效果就会越好。因此首先，尽可能多地寻找优质数据集。
Data Augmentation 而如果在数据集不够时，对数据的预处理就显得十分重要，对原先数据的处理扩增被称作 data augmentation，方法主要是下面的几种：
对于语音或者图像，可以在原本的数据上加噪声，或者进行缩放，镜像等操作； 对于数值类向量，可以在原先的数据基础上进行随机； 文字类数据，我觉得并不会缺少数据集（笑）。 在数据中添加噪声，或者平移缩放等操作在很多比赛以及网络中已经证明了，可以很好地提高模型的泛化能力。
数据归一化 这是一个老生常谈的问题了，提前把数据进行归一化可以很好地防止梯度爆炸。在网络中，我们也可以设置激活函数（sigmoid、tanh等）对数据进行归一化。而激活函数也容易造成梯度消失的问题，我们可以更换激活函数，或者进行 normalize，这会在另一篇文章里面讨论。
数据本身 最后还有一个方法，就是从数据本身寻找问题。举个例子，老师职位和性别有关吗？车速和天气有关系吗？答案是否定的，所以要检查数据集中的某些特征是不是与我们要预测或分类的特征毫无关联，如果是的话就尽量抛弃。
另外，如果有其他的特征或者更加适合的方法，也需要对数据进行调整。
算法 其实在我的很多情况下，当无法采取某种特定的算法去进行数据处理的时候，便会采用深度学习其训练拟合（笑）。
选择算法 什么算法对于特定的问题效果最好，我们是没有办法也不可能知道的，只能取范围内最优，即当前使用的算法并不一定适合当前的问题。
如果没有头绪的话，可以从下面几点进行尝试：
线性算法，如逻辑回归和线性判断分析 树模型，如 CART、随机森林等 SVM 或 KNN 等算法 神经网络模型，如 CNN、RNN、LSTM 然后根据经验和结果综合判断，选择目标模型进行调参或者优化，从而进一步提升效果。
之前比赛的时候有一个大佬的解法是训练两个网络进行对抗，这个算法拿了准确率第一，当时印象挺深刻的。
算法调优 调参是一个算法调优的必经之路，主要分为以下几点：
模型可诊断性，模型总是处于两种状态，即欠拟合和过拟合之间，只是程度不同罢了。我们可以把 accuracy 和 loss 输出在图表中，从而作为一个有价值的诊断工具。 权重的初始化，用小的随机数初始化权重。 学习率，学习率决定了梯度下降时的范围，如果添加了更多的神经节点和网络层，就可加大学习率。这一点并不是独立的，与 batch size 等都有关系。 激活函数，激活函数其实有很多种，而且每一种激活函数都有一段流行的时间，如今一般都会使用 ReLU 函数，但 ReLU 本身存在着死神经元的问题，然后又诞生了 Leaky ReLU。尝试不同的激活函数可能会有不同的效果。 网络结构，网络的结构设计也会很大程度影响性能，在一般情况下，最好使用成熟的网络，或者尝试发表论文中的网络。 batch size 和 epoch，batch size 决定了梯度值以及权重的更新频率，一般设置的越大计算的速度就越快，反之越慢；epoch 则是样本参与训练的循环次数。 正则项，这可以很好地解决过拟合，如 dropout。 优化方法和损失函数，现在在梯度下降的过程中有许多优化器。默认的梯度下降方法是得到一个结果，然后调整动量值、学习率进行优化，而复杂的优化器则会有更多的参数设置，这需要多尝试去积累经验。 提早结束训练，一旦效果变差了，可以及时停止优化训练。而这一部分可以直接通过 TensorFlow 或者 Pytorch 框架中的 checkpoint 功能，自动选择和保存相应的模型训练点。 宏观调整 最后边是调出神经网络这个视角，从宏观的角度出发，把神经网络作为一个个体去看待问题。</description>
    </item>
    
  </channel>
</rss>
